---
title: "apple morphometric analysis"
format: 
  html:
    fig-format: pdf 
editor: visual
editor_options:
  chunk_output_type: console
---

<!-- Preliminary modules, libraries and functions for the rest of the code to function -->

```{r}
#| label: R Libraries
#| echo: false
#########################
### LOAD IN LIBRARIES ###
#########################

# python libraries that will be used in the python sections
# test to make sure these will load
library(reticulate)
#py_install("opencv-python")
#py_install("scikit-learn")
#py_install("matplotlib")
#py_install("pandas")
#py_install("numpy")
#py_install("seaborn")

# other r libraries
library(tidyverse) # used for ggplot2
```

```{python}
#| label: Python Libraries
#| echo: false
#######################
### LOAD IN MODULES ###
#######################

import cv2 # to install on mac: pip install opencv-python
from scipy.interpolate import interp1d # for interpolating points
from sklearn.decomposition import PCA # for principal component analysis
from scipy.spatial import procrustes # for Procrustes analysis
from scipy.spatial import ConvexHull # for convex hull
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA
from sklearn.metrics import confusion_matrix # for confusion matrix
from os import listdir # for retrieving files from directory
from os.path import isfile, join # for retrieving files from directory
import matplotlib.pyplot as plt # for plotting
import numpy as np # for using arrays
import math # for mathematical operations
import pandas as pd # for using pandas dataframes
import seaborn as sns # for plotting in seaborn
from matplotlib.colors import LogNorm
```

```{python}
#| label: Python Functions
#| echo: false
#################
### FUNCTIONS ###
#################

def angle_between(p1, p2, p3):
    """
    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex
    inputs: three angle points, as tuples
    output: angle in degrees
    """
    x1, y1 = p1
    x2, y2 = p2
    x3, y3 = p3
    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360
    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360
    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)

def rotate_points(xvals, yvals, degrees):
    """"
    define a function to rotate 2D x and y coordinate points around the origin
    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate
    outputs: rotated and y vals
    """
    angle_to_move = 90-degrees
    rads = np.deg2rad(angle_to_move)
    
    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)
    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)
    
    return new_xvals, new_yvals

def interpolation(x, y, number): 
    """
    define a function to return equally spaced, interpolated points for a given polyline
    inputs: arrays of x and y values for a polyline, number of points to interpolate
    ouputs: interpolated points along the polyline, inclusive of start and end points
    """
    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))
    distance = distance/distance[-1]

    fx, fy = interp1d( distance, x ), interp1d( distance, y )

    alpha = np.linspace(0, 1, number)
    x_regular, y_regular = fx(alpha), fy(alpha)
    
    return x_regular, y_regular

def euclid_dist(x1, y1, x2, y2):
    """
    define a function to return the euclidean distance between two points
    inputs: x and y values of the two points
    output: the eulidean distance
    """
    return np.sqrt((x2-x1)**2 + (y2-y1)**2)

def poly_area(x,y):
    """
    define a function to calculate the area of a polygon using the shoelace algorithm
    inputs: separate numpy arrays of x and y coordinate values
    outputs: the area of the polygon
    """
    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))

def gpa_mean(leaf_arr, landmark_num, dim_num):
    
    """
    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean
    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions
    output: an array of the Generalized Procrustes Analysis mean shape
    
    """

    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to
    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape

    mean_diff = 10**(-30) # set a distance between means to stop the algorithm

    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape

    d = 1000000 # set d initially arbitraily high

    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations

        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals

        for i in range(len(leaf_arr)): # for each leaf shape 

            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf
            arr[i] = s2 # store procrustes adjusted shape to array

        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference

        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean

        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration

    return new_mean
```

<!-- Following sections are from the domestic apples. Reads data and does the preliminary image analysis -->

### Apple Data

#### Metadata Listing

```{python}
#| label: Apple Metadata
########################
### READ IN METADATA ###
########################

# CHANGE TO CORRECT DIRECTORY
mdata = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/apple_metadata.csv") # read in csv

mdata.head() # head data to check
```

```{python}
#| label: Apple List
#######################################
### MAKE A LIST OF IMAGE FILE NAMES ###
#######################################

# CHANGE TO CORRECT DIRECTORY
data_dir = "C:/Users/User/Desktop/COOP02/apples/binary_side/" # set data directory 

file_names = [f for f in listdir(data_dir) if isfile(join(data_dir, f))] # create a list of file names

#file_names.remove('.DS_Store') # remove .DS_Store file

file_names.sort() # sort the list of file names

file_names[slice(50)] # check list of file names, can put file_name[slice(50)]
```

#### Process and Landmarking

-   Read in image in grayscale
-   Select the contour of the largest object (the leaf)
-   Interpolate with a high resolution of pseudo-landmarks
-   Find the bottom and top index point on the high resolution contour
-   Reset the bottom index to zero
-   Interpolate each side with desired number of equidistant pseudo-landmarks
-   Rotate leaves and scale to centimeters
-   Save pseudo-landmarks scaled to centimeters in an array

##### PARAMETERS AND INDEXING:

-   `high_res_pts` is an arbitrarily high number of points to initially interpolate
-   `res` is the desired number of points to interpolate on each side of the leaf
-   The total number of pseudo-landmarks will be `2*res - 1`
-   The bottom index will be `0`
-   The top index will be `res-1`
-   The returned apples in `apple_cm_arr` are scaled in size to centimeters

```{python}
#| label: Apple Parameters
######################
### SET PARAMETERS ###
######################

# the number of equidistant points to create
# an initial high resolution outline of the leaf
high_res_pts = 1000 

# the ultimate number of equidistant points on each side of the leaf
# (-1 for the top)
# the leaf will have res*2-1 pseudo-landmarks
#################
#################
#################
res = 20 ########
#################
#################
#################

# an array to store pseudo-landmarks
apple_cm_arr = np.zeros((len(mdata),(res*2)-1,2))

# for each apple . . .
for lf in range(len(mdata)):

    ###############################
    ### READ IN GRAYSCALE IMAGE ###
    ###############################

    curr_image = mdata["file"][lf] # select the current image
    print(lf, curr_image) # print each leaf in case there are problems later
    
    # read in image
    # convert to grayscale
    # invert the binary
    img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(data_dir + curr_image),cv2.COLOR_BGR2GRAY))

    # find contours of binary objects
    contours, hierarchy = cv2.findContours(img,  
        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    ##############################
    ### SELECT LARGEST CONTOUR ###
    ##############################

    # ideally there is only one leaf in the image
    # in the case there are smaller objects
    # this code selects the largest object (the apple)
    # if there is one and only one object in the image
    # then the following code is not necessary

    x_conts = [] # list of lists of contour x vals
    y_conts = [] # list of lists of contour y vals
    areas_conts = [] # list of bounding box areas of contours
    for c in contours: # for each contour
        x_vals = [] # store x vals for current contour 
        y_vals = [] # store y vals for current contour
        for i in range(len(c)): # for each point in current contour
            x_vals.append(c[i][0][0]) # isolate x val
            y_vals.append(c[i][0][1]) # isolate y val
        area = (max(x_vals) - min(x_vals))*(max(y_vals) - min(y_vals)) # calculate bounding box area of contour
        x_conts.append(x_vals) # append the current contour x vals
        y_conts.append(y_vals) # append the current contour y vals
        areas_conts.append(area) # append the current contour bounding box areas

    area_inds = np.flip(np.argsort(areas_conts)) # get indices to sort contours by area
    sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, x vals
    sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, y vals

    ################################################
    ### INTERPOLATE HIGH RES NUMBER OF LANDMARKS ###
    ################################################

    # convert the leaf to high resolution number of landmarks
    # using high_res_pt value
    # need to convert arrays of pixel int to floats first
    high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32), 
                                           np.array(sorted_y_conts[0], dtype=np.float32), high_res_pts)

    #################################
    ### FIND BOTTOM AND TOP INDEX ###
    #################################

    # get the bottom and top landmark point values
    bottom_pt = np.array((mdata["bottom_x"][lf], mdata["bottom_y"][lf]))
    top_pt = np.array((mdata["top_x"][lf], mdata["top_y"][lf]))

    bottom_dists = [] # store distance of each high res point to bottom
    top_dists = [] # store distance of each high res point to top

    for pt in range(len(high_res_x)): # for each of the high resolution points

        # euclidean distance of the current point from the bottom and top landmark
        ed_bottom = euclid_dist(bottom_pt[0], bottom_pt[1], high_res_x[pt], high_res_y[pt])
        ed_top = euclid_dist(top_pt[0], top_pt[1], high_res_x[pt], high_res_y[pt])

        # store distance of current point from bottom/top
        bottom_dists.append(ed_bottom)
        top_dists.append(ed_top)

    # get index of bottom and top points
    bottom_ind = np.argmin(bottom_dists)
    top_ind = np.argmin(top_dists)

    ##################################
    ### RESET BOTTOM INDEX TO ZERO ###
    ##################################

    # reset bottom index position to zero
    high_res_x = np.concatenate((high_res_x[bottom_ind:],high_res_x[:bottom_ind]))
    high_res_y = np.concatenate((high_res_y[bottom_ind:],high_res_y[:bottom_ind]))

    # recalculate indices with new indexing
    top_ind = top_ind-bottom_ind # note: negative index if top_ind<bottom_ind
    bottom_ind = bottom_ind-bottom_ind

    # create single array for leaf coordinates
    lf_contour = np.column_stack((high_res_x, high_res_y))

    ##############################################################
    ### INTERPOLATE EACH SIDE WITH DESIRED NUMBER OF LANDMARKS ###
    ##############################################################

    # interpolate at desired resolution the left and right sides of the apple
    left_inter_x, left_inter_y = interpolation(lf_contour[bottom_ind:top_ind+1,0],lf_contour[bottom_ind:top_ind+1,1],res)
    right_inter_x, right_inter_y = interpolation(lf_contour[top_ind:,0],lf_contour[top_ind:,1],res)

    # the start of the right side and end of the left side
    # both contain the top landmark
    # delete the last point on the left side
    left_inter_x = np.delete(left_inter_x, -1)
    left_inter_y = np.delete(left_inter_y, -1)

    # BOTTOM OF APPLE IS INDEX 0
    # TOP INDEX IS RES-1 IF BOTH LEFT & RIGHT POINTS
    # TOTAL PSEUDOLANDMARKS IS 2*RES-1
    lf_pts_left = np.column_stack((left_inter_x, left_inter_y))
    lf_pts_right = np.column_stack((right_inter_x, right_inter_y))
    lf_pts = np.row_stack((lf_pts_left, lf_pts_right))

    ##########################################################
    ### ROTATE APPLES UPWARD AND SCALE SIZE TO CENTIMETERS ###
    ##########################################################

    top_point = lf_pts[res-1,:] # get top point
    bottom_point = lf_pts[0,:] # get bottom point

    # calculate angle between top. bottom, and an arbitrary reference
    ang = angle_between(top_point, bottom_point, (bottom_point[0]+1,bottom_point[1]) )

    # rotate points upwards
    rot_x, rot_y = rotate_points(lf_pts[:,0], lf_pts[:,1], ang) 
    rot_pts = np.column_stack((rot_x, rot_y))
    
    # calculate leaf area in pixels^2
    lf_area_px2 = poly_area(rot_pts[:,0], rot_pts[:,1])
    
    # get px_cm
    #px_cm = np.sqrt(lf_area_px2/mdata["area"][lf])

    # scale leaf into cm
    cm_lf = rot_pts/(81.93)
    
    # store the leaf scaled into cm into the cm_arr
    apple_cm_arr[lf,:,:] = cm_lf
```

<!-- Plot to be taken out for final analysis -->

### Only used for testing

```{python}
#| label: Pseudo-landmarks
# Plot each leaf and check that it is working
# (remove this when working with a large number of leaves outside of this example)

plt.figure(figsize=(5,10))


for i in range(25):
    
    plt.subplot(5,5,i+1)
    plt.plot(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", lw=1)
    plt.plot([min(apple_cm_arr[i,:,0])-0.1,min(apple_cm_arr[i,:,0])-0.1],
            [apple_cm_arr[i,0,1], apple_cm_arr[i,0,1]+1], c="k", lw=0.5) # cm scale
    plt.scatter(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", s=4)
    plt.scatter(apple_cm_arr[i,0,0], apple_cm_arr[i,0,1])
    plt.scatter(apple_cm_arr[i,res-1,0], apple_cm_arr[i,res-1,1])
    
    plt.title(mdata["apple_id"][i], fontsize=8)
    
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
plt.suptitle(str(res*2-1) + " pseudo-landmarks")
plt.tight_layout()
```

<!-- The following sections of code are from the analysis -->

### Analyzing Apple Dimensions

Using placed pseudo-landmarks representing leaves that are rotated upwards and scaled in centimeters from `cm_arr`, calculate the following:

-   `width`: difference in centimeters between minimum and maximum x values in an oriented apple
-   `length`: difference in centimeters between minimum and maximum y values in an oriented apple
-   `area`: area of the apple in centimeters squared
-   `solidity`: the ratio of area to convex hull area
-   `asymmetry`: the Procrustes distance between the superimposed left and right sides of a apple outline. Lower values are more symmetric. Higher values are more asymmetric.

Data is stored in the `mdata` dataframe.

```{python}
#| label: Analyzing Apple Dimensions
# lists to store variables
width_list = []
length_list = []
area_list = []
solidity_list = []
asymmetry_list = []

# for each apple . . .
for lf in range(len(apple_cm_arr)):
    
    # for calculating dimensions, we need non-scaled apple in centimeters
    curr_lf = apple_cm_arr[lf,:,:] # select current leaf
    
    ############################
    ### CALCULATE DIMENSIONS ###
    ############################
    
    width = np.max(curr_lf[:,0])-np.min(curr_lf[:,0]) # calculate width
    length = np.max(curr_lf[:,1])-np.min(curr_lf[:,1]) # calculate length
    area = poly_area(curr_lf[:,0],curr_lf[:,1]) # calcualte area
    
    ##########################
    ### CALCULATE SOLIDITY ###
    ##########################
    
    hull = ConvexHull(curr_lf) # calculate convex hull of current leaf
    vertices = hull.vertices # isolate vertex indices of convex hull
    convex_area = poly_area(curr_lf[vertices,0], curr_lf[vertices,1]) # calculate convex area
    solidity = area / convex_area # calculate solidity
    
    ##########################
    ### CALCULATE SYMMETRY ###
    ##########################
    
    left_side = curr_lf[:(res-1)+1,] # isolate left side of leaf
    right_side = curr_lf[(res-1):,] # isolate right side of leaf
    right_side = right_side[::-1] # reverse the right side to align indices with left

    # calculate procrustes distance between left and right side of leaf
    s1, s2, distance = procrustes(left_side, right_side) 
    
    # store data in lists
    width_list.append(width)
    length_list.append(length)
    area_list.append(area)
    solidity_list.append(solidity)
    asymmetry_list.append(distance)
    
# add data to the mdata dataframe
mdata["width"] = width_list
mdata["length"] = length_list
mdata["area"] = area_list
mdata["solidity"] = solidity_list
mdata["asymmetry"] = asymmetry_list

# save output so you don't have to run the code all the time
#mdata.to_csv("added_data.csv", index = False)
```

<!-- PLEASE INSERT PLOT BELOW -->

<!-- NOTE: will need to convert datasets that will be used from python to r. This can be done by using the py_to_r function from the reticulate library or import a file directly into R -->

```{r}
# insert pair plot here to visualize the differences in width, length, area, solidity, asymmetry

# load libraries
library(tidyverse)
library(GGally)

apple_data <- read.csv("added_data.csv")
apple_data$apple_id <- factor(apple_data$apple_id)

ggpairs(apple_data, columns = 11:15, aes(colour = apple_id, alpha = 0.5), upper = list(continuous = "points"))
```

<!-- The following sections are the Procrustes Analysis -->

### Procrustes Analysis

Perform a Procrustes analysis to translate, scale, and rotate leaf shapes

-   Select number of pseudo-landmarks and dimensions
-   Calculate the GPA mean leaf shape using the `gpa_mean` function
-   Align all leaves to the GPA mean
-   Store Procrustes super-imposed leaves in an array, `proc_arr`
-   Calculate a PCA for all possible axes and their variance (the number of leaves)
-   Calculate a PCA for just the axes needed for reconstruction of eigenleaves for morphospace (probably 2)

#### Calculate GPA Mean

```{python}
#| label: GPA Mean
landmark_num = (res*2)-1 # select number of landmarks
dim_num = 2 # select number of coordinate value dimensions

##########################
### CALCULATE GPA MEAN ###
##########################

mean_shape = gpa_mean(apple_cm_arr, landmark_num, dim_num)

################################
### ALIGN LEAVES TO GPA MEAN ###
################################

# array to store Procrustes aligned shapes
proc_arr = np.zeros(np.shape(apple_cm_arr)) 

for i in range(len(apple_cm_arr)):
  s1, s2, distance = procrustes(mean_shape, apple_cm_arr[i,:,:]) # calculate procrustes adjusted shape to ref for current leaf
  proc_arr[i] = s2 # store procrustes adjusted shape to array
```

#### Calculate PC

```{python}
#| label: Calculate PC
#################################################
### FIRST, CALCULATE PERCENT VARIANCE ALL PCs ###
#################################################

######
PC_NUMBER = 78 # PC number = not number of leaves, but features
#######

# use the reshape function to flatten to 2D
flat_arr = proc_arr.reshape(np.shape(proc_arr)[0], 
                                 np.shape(proc_arr)[1]*np.shape(proc_arr)[2]) 

pca_all = PCA(n_components=PC_NUMBER) 
PCs_all = pca_all.fit_transform(flat_arr) # fit a PCA for all data

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca_all.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca_all.explained_variance_ratio_[i]*100,1)) + 
          "%, " + str(round(pca_all.explained_variance_ratio_.cumsum()[i]*100,1)) + "%"  )
```

```{python}
#| label: Calculate Number of PCs
#################################################
### NEXT, CALCULATE THE DESIRED NUMBER OF PCs ###
#################################################

######
PC_NUMBER = 3 # PC number = 3, because PC 2 is asymmetry and we want to create morphospace for PC = 1 and 3
#######

pca = PCA(n_components=PC_NUMBER) 
PCs = pca.fit_transform(flat_arr) # fit a PCA for only desired PCs

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca.explained_variance_ratio_[i]*100,1)) + 
          "%, " + str(round(pca.explained_variance_ratio_.cumsum()[i]*100,1)) + "%"  )
    
# add PCs to dataframe for plotting
## MAY WANT TO USE MORE THAN 3 PCS
mdata["PC1"] = PCs[:,0]
mdata["PC2"] = PCs[:,1]
mdata["PC3"] = PCs[:,2]

# save output with PC, could over write the other one if desired
#mdata.to_csv("added_data2.csv", index = False)
```

#### Eigen Representation of PC

```{python}
#| label: Eigen Representation of PC
###################################################
### CREATE EIGEN REPRESENTATIONS OF FIRST 3 PCs ###
###################################################


# calculate standard deviations for each PC
PC1_std = mdata["PC1"].std()
PC2_std = mdata["PC2"].std()
PC3_std = mdata["PC3"].std()


# create list of lists of PC values to reconstruct

PC_vals = [[-2*PC1_std,0,0],
           [-1*PC1_std,0,0],
           [0*PC1_std,0,0],
           [1*PC1_std,0,0],
           [2*PC1_std,0,0],
           [0,-2*PC2_std,0],
           [0,-1*PC2_std,0],
           [0,0*PC2_std,0],
           [0,1*PC2_std,0],
           [0,2*PC2_std,0],
           [0,0,-2*PC3_std],
           [0,0,-1*PC3_std],
           [0,0,0*PC3_std],
           [0,0,1*PC3_std],
           [0,0,2*PC3_std] ]

plt.figure(figsize=(5,5))

counter = 1

for i in range(len(PC_vals)):
    
    # create inverse leaf
    inv_leaf = pca.inverse_transform(np.array(PC_vals[i]))
    inv_x = inv_leaf[0::2] # select just inverse x vals
    inv_y = inv_leaf[1::2] # select just inverse y vals
    
    # plot inverse leaf
    plt.subplot(3,5,counter)
    plt.fill(inv_x, inv_y, c="lightgray")
    plt.plot(inv_x, inv_y, c="gray")
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
    
    counter += 1

plt.tight_layout()
```

### Morphospace and Linear Discriminant Analysis (By Genotype)

Visualize a morphospace and classify leaves by the factor of genotype

-   Plot a morphospace using the inverse transform of the PCA
-   Perform a Linear Discriminant Analysis
-   Visualize LDA results as a confusion matrix
-   Plot out LDA scores
-   Note: the number of LDs is 1 minus the number of factor levels. In this example there are two genotypes, so there is only 1 LD
-   Note: the column names to plot the Linear Discriminants are manually renamed

#### Creating Morphospace

```{python}
#| label: Morphospace (PC1 and PC3)
##########################
### CREATE MORPHOSPACE ###
##########################

# set plot parameters 
mdata['apple_id'] = mdata['apple_id'].astype('category')


plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 10 # set number of PC1 intervals
numPC3 = 5 # set number of PC3 intervals
hue = "apple_id" # select the factor to color by
s = 0.075 # set the scale of the eigenleaves
lf_col = "lightgray" # color of inverse eigenleaf
lf_alpha = 0.5 # alpha of inverse eigenleaf
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC3_vals = np.linspace( np.min(PCs[:,2]), np.max(PCs[:,2]), numPC3 )

for i in PC1_vals: # for each PC1 interval
    for j in PC3_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc3_val = j # select the current PC3 val

        # calculate the inverse eigenleaf
        inv_leaf = pca.inverse_transform(np.array([pc1_val,0,pc3_val]))
        inv_x = inv_leaf[0::2] # select just inverse x vals
        inv_y = inv_leaf[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc3_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=mdata, x="PC1", y="PC3", hue=hue, s=pt_size, linewidth=pt_linewidth, alpha=pt_alpha)

# legend not helpful in this case
# plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9}, ncol=2)
plt.gca().get_legend().remove()
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC3, " + str(round(pca.explained_variance_ratio_[2]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
```

```{python}
#| label: Morphospace (PC1 and PC2)
##########################
### CREATE MORPHOSPACE ###
##########################

# set plot parameters 
mdata['apple_id'] = mdata['apple_id'].astype('category')


plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 10 # set number of PC1 intervals
numPC2 = 5 # set number of PC3 intervals
hue = "apple_id" # select the factor to color by
s = 0.075 # set the scale of the eigenleaves
lf_col = "lightgray" # color of inverse eigenleaf
lf_alpha = 0.5 # alpha of inverse eigenleaf
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC2_vals = np.linspace( np.min(PCs[:,1]), np.max(PCs[:,1]), numPC2 )

for i in PC1_vals: # for each PC1 interval
    for j in PC2_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc2_val = j # select the current PC3 val

        # calculate the inverse eigenleaf
        inv_leaf = pca.inverse_transform(np.array([pc1_val,0,pc2_val]))
        inv_x = inv_leaf[0::2] # select just inverse x vals
        inv_y = inv_leaf[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc2_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=mdata, x="PC1", y="PC2", hue=hue, s=pt_size, linewidth=pt_linewidth, alpha=pt_alpha)

# legend not helpful in this case
# plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9}, ncol=2)
plt.gca().get_legend().remove()
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC2, " + str(round(pca.explained_variance_ratio_[1]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
```

```{python}
# To analyze leaf size/allometry
# calculate leaf area of each leaf in cm
# and replace 'area' column values in combined_df

leaf_areas_cm = []

for lf in range(len(apple_cm_arr)):
    
    leaf_areas_cm.append(poly_area(apple_cm_arr[lf,:,0], apple_cm_arr[lf,:,1]))
    
mdata["node"] = leaf_areas_cm

# export this one with the updated area
mdata.to_csv("added_data_update.csv", index = False)
```

```{python}
#| label: Morphospace 2
##########################
### CREATE MORPHOSPACE ###
##########################

# set plot parameters

plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 10 # set number of PC1 intervals
numPC3 = 5 # set number of PC3 intervals
hue = "node" # select the factor to color by
s = 0.075 # set the scale of the eigenleaves
lf_col = "lightgray" # color of inverse eigenleaf
lf_alpha = 0.5 # alpha of inverse eigenleaf
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC3_vals = np.linspace( np.min(PCs[:,2]), np.max(PCs[:,2]), numPC3 )

for i in PC1_vals: # for each PC1 interval
    for j in PC3_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc3_val = j # select the current PC3 val

        # calculate the inverse eigenleaf
        inv_leaf = pca.inverse_transform(np.array([pc1_val,0,pc3_val]))
        inv_x = inv_leaf[0::2] # select just inverse x vals
        inv_y = inv_leaf[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc3_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=mdata, 
                x="PC1", 
                y="PC3", 
                hue=hue, 
                s=pt_size, 
                linewidth=pt_linewidth, 
                alpha=pt_alpha, 
                palette = "inferno_r")

plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9})
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC3, " + str(round(pca.explained_variance_ratio_[2]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
```

<!-- Update the metadata?-->


<!-- Linear Discriminant Analysis to be completed here (can be done in R): -->

<!-- Plots that may want to be analyzed: score by genotype, score by size, etc. -->

```{r}

```
