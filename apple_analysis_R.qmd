---
title: "apple morphometric analysis (side view)"
format: 
  html:
    fig-format: pdf 
editor: visual
---

<!-- Preliminary modules, libraries and functions for the rest of the code to function -->

```{r}
#| label: R Libraries
#| echo: false
#########################
### LOAD IN LIBRARIES ###
#########################

# python libraries that will be used in the python sections
# test to make sure these will load
library(reticulate)
#py_install("opencv-python")
#py_install("scikit-learn")
#py_install("matplotlib")
#py_install("pandas")
#py_install("numpy")
#py_install("seaborn")

# other r libraries
library(tidyverse) # used for ggplot2
```

```{python}
#| label: Python Libraries
#| echo: false
#######################
### LOAD IN MODULES ###
#######################

import cv2 # to install on mac: pip install opencv-python
from scipy.interpolate import interp1d # for interpolating points
from sklearn.decomposition import PCA # for principal component analysis
from scipy.spatial import procrustes # for Procrustes analysis
from scipy.spatial import ConvexHull # for convex hull
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # for LDA
from sklearn.metrics import confusion_matrix # for confusion matrix
from os import listdir # for retrieving files from directory
from os.path import isfile, join # for retrieving files from directory
import matplotlib.pyplot as plt # for plotting
import numpy as np # for using arrays
import math # for mathematical operations
import pandas as pd # for using pandas dataframes
import seaborn as sns # for plotting in seaborn
from matplotlib.colors import LogNorm
```

```{python}
#| label: Python Functions
#| echo: false
#################
### FUNCTIONS ###
#################

def angle_between(p1, p2, p3):
    """
    define a function to find the angle between 3 points anti-clockwise in degrees, p2 being the vertex
    inputs: three angle points, as tuples
    output: angle in degrees
    """
    x1, y1 = p1
    x2, y2 = p2
    x3, y3 = p3
    deg1 = (360 + math.degrees(math.atan2(x1 - x2, y1 - y2))) % 360
    deg2 = (360 + math.degrees(math.atan2(x3 - x2, y3 - y2))) % 360
    return deg2 - deg1 if deg1 <= deg2 else 360 - (deg1 - deg2)

def rotate_points(xvals, yvals, degrees):
    """"
    define a function to rotate 2D x and y coordinate points around the origin
    inputs: x and y vals (can take pandas dataframe columns) and the degrees (positive, anticlockwise) to rotate
    outputs: rotated and y vals
    """
    angle_to_move = 90-degrees
    rads = np.deg2rad(angle_to_move)
    
    new_xvals = xvals*np.cos(rads)-yvals*np.sin(rads)
    new_yvals = xvals*np.sin(rads)+yvals*np.cos(rads)
    
    return new_xvals, new_yvals

def interpolation(x, y, number): 
    """
    define a function to return equally spaced, interpolated points for a given polyline
    inputs: arrays of x and y values for a polyline, number of points to interpolate
    ouputs: interpolated points along the polyline, inclusive of start and end points
    """
    distance = np.cumsum(np.sqrt( np.ediff1d(x, to_begin=0)**2 + np.ediff1d(y, to_begin=0)**2 ))
    distance = distance/distance[-1]

    fx, fy = interp1d( distance, x ), interp1d( distance, y )

    alpha = np.linspace(0, 1, number)
    x_regular, y_regular = fx(alpha), fy(alpha)
    
    return x_regular, y_regular

def euclid_dist(x1, y1, x2, y2):
    """
    define a function to return the euclidean distance between two points
    inputs: x and y values of the two points
    output: the eulidean distance
    """
    return np.sqrt((x2-x1)**2 + (y2-y1)**2)

def poly_area(x,y):
    """
    define a function to calculate the area of a polygon using the shoelace algorithm
    inputs: separate numpy arrays of x and y coordinate values
    outputs: the area of the polygon
    """
    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))

def gpa_mean(leaf_arr, landmark_num, dim_num):
    
    """
    define a function that given an array of landmark data returns the Generalized Procrustes Analysis mean
    inputs: a 3 dimensional array of samples by landmarks by coordinate values, number of landmarks, number of dimensions
    output: an array of the Generalized Procrustes Analysis mean shape
    
    """

    ref_ind = 0 # select arbitrary reference index to calculate procrustes distances to
    ref_shape = leaf_arr[ref_ind, :, :] # select the reference shape

    mean_diff = 10**(-30) # set a distance between means to stop the algorithm

    old_mean = ref_shape # for the first comparison between means, set old_mean to an arbitrary reference shape

    d = 1000000 # set d initially arbitraily high

    while d > mean_diff: # set boolean criterion for Procrustes distance between mean to stop calculations

        arr = np.zeros( ((len(leaf_arr)),landmark_num,dim_num) ) # empty 3D array: # samples, landmarks, coord vals

        for i in range(len(leaf_arr)): # for each leaf shape 

            s1, s2, distance = procrustes(old_mean, leaf_arr[i]) # calculate procrustes adjusted shape to ref for current leaf
            arr[i] = s2 # store procrustes adjusted shape to array

        new_mean = np.mean(arr, axis=(0)) # calculate mean of all shapes adjusted to reference

        s1, s2, d = procrustes(old_mean, new_mean) # calculate procrustes distance of new mean to old mean

        old_mean = new_mean # set the old_mean to the new_mea before beginning another iteration

    return new_mean
```

<!-- Following sections are from the domestic apples. Reads data and does the preliminary image analysis -->

### Apple Data

#### Metadata Listing

```{python}
#| label: Apple Metadata
########################
### READ IN METADATA ###
########################

# CHANGE TO CORRECT DIRECTORY
mdata = pd.read_csv("C:/Users/User/Desktop/COOP02/code/apple_shape/apple_metadata.csv") # read in csv

mdata.head() # head data to check
```

```{python}
#| label: Apple List
#######################################
### MAKE A LIST OF IMAGE FILE NAMES ###
#######################################

# CHANGE TO CORRECT DIRECTORY
data_dir = "C:/Users/User/Desktop/COOP02/code/apple_shape/binary_domestica/" # set data directory 

file_names = [f for f in listdir(data_dir) if isfile(join(data_dir, f))] # create a list of file names

#file_names.remove('.DS_Store') # remove .DS_Store file

file_names.sort() # sort the list of file names

file_names[slice(50)] # check list of file names, can put file_name[slice(50)]
```

#### Process and Landmarking

-   Read in image in grayscale
-   Select the contour of the largest object (the leaf)
-   Interpolate with a high resolution of pseudo-landmarks
-   Find the bottom and top index point on the high resolution contour
-   Reset the bottom index to zero
-   Interpolate each side with desired number of equidistant pseudo-landmarks
-   Rotate leaves and scale to centimeters
-   Save pseudo-landmarks scaled to centimeters in an array

##### PARAMETERS AND INDEXING:

-   `high_res_pts` is an arbitrarily high number of points to initially interpolate
-   `res` is the desired number of points to interpolate on each side of the leaf
-   The total number of pseudo-landmarks will be `2*res - 1`
-   The bottom index will be `0`
-   The top index will be `res-1`
-   The returned apples in `apple_cm_arr` are scaled in size to centimeters

```{python}
#| label: Apple Parameters
######################
### SET PARAMETERS ###
######################

# the number of equidistant points to create
# an initial high resolution outline of the leaf
high_res_pts = 1000 

# the ultimate number of equidistant points on each side of the apple
# (-1 for the top)
# the apple will have res*2-1 pseudo-landmarks
#################
#################
#################
res = 20 ########
#################
#################
#################

# an array to store pseudo-landmarks
apple_cm_arr = np.zeros((len(mdata),(res*2)-1,2))

# for each apple . . .
for lf in range(len(mdata)):

    ###############################
    ### READ IN GRAYSCALE IMAGE ###
    ###############################

    curr_image = mdata["file"][lf] # select the current image
    print(lf, curr_image) # print each leaf in case there are problems later
    
    # read in image
    # convert to grayscale
    # invert the binary
    img = cv2.bitwise_not(cv2.cvtColor(cv2.imread(data_dir + curr_image),cv2.COLOR_BGR2GRAY))

    # find contours of binary objects
    contours, hierarchy = cv2.findContours(img,  
        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    ##############################
    ### SELECT LARGEST CONTOUR ###
    ##############################

    # ideally there is only one apple in the image
    # in the case there are smaller objects
    # this code selects the largest object (the apple)
    # if there is one and only one object in the image
    # then the following code is not necessary

    x_conts = [] # list of lists of contour x vals
    y_conts = [] # list of lists of contour y vals
    areas_conts = [] # list of bounding box areas of contours
    for c in contours: # for each contour
        x_vals = [] # store x vals for current contour 
        y_vals = [] # store y vals for current contour
        for i in range(len(c)): # for each point in current contour
            x_vals.append(c[i][0][0]) # isolate x val
            y_vals.append(c[i][0][1]) # isolate y val
        area = (max(x_vals) - min(x_vals))*(max(y_vals) - min(y_vals)) # calculate bounding box area of contour
        x_conts.append(x_vals) # append the current contour x vals
        y_conts.append(y_vals) # append the current contour y vals
        areas_conts.append(area) # append the current contour bounding box areas

    area_inds = np.flip(np.argsort(areas_conts)) # get indices to sort contours by area
    sorted_x_conts = np.array(x_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, x vals
    sorted_y_conts = np.array(y_conts, dtype=object)[area_inds][0:] # areas sorted largest to smallest, y vals

    ################################################
    ### INTERPOLATE HIGH RES NUMBER OF LANDMARKS ###
    ################################################

    # convert the apple to high resolution number of landmarks
    # using high_res_pt value
    # need to convert arrays of pixel int to floats first
    high_res_x, high_res_y = interpolation(np.array(sorted_x_conts[0], dtype=np.float32), 
                                           np.array(sorted_y_conts[0], dtype=np.float32), high_res_pts)

    #################################
    ### FIND BOTTOM AND TOP INDEX ###
    #################################

    # get the bottom and top landmark point values
    bottom_pt = np.array((mdata["bottom_x"][lf], mdata["bottom_y"][lf]))
    top_pt = np.array((mdata["top_x"][lf], mdata["top_y"][lf]))

    bottom_dists = [] # store distance of each high res point to bottom
    top_dists = [] # store distance of each high res point to top

    for pt in range(len(high_res_x)): # for each of the high resolution points

        # euclidean distance of the current point from the bottom and top landmark
        ed_bottom = euclid_dist(bottom_pt[0], bottom_pt[1], high_res_x[pt], high_res_y[pt])
        ed_top = euclid_dist(top_pt[0], top_pt[1], high_res_x[pt], high_res_y[pt])

        # store distance of current point from bottom/top
        bottom_dists.append(ed_bottom)
        top_dists.append(ed_top)

    # get index of bottom and top points
    bottom_ind = np.argmin(bottom_dists)
    top_ind = np.argmin(top_dists)

    ##################################
    ### RESET BOTTOM INDEX TO ZERO ###
    ##################################

    # reset bottom index position to zero
    high_res_x = np.concatenate((high_res_x[bottom_ind:],high_res_x[:bottom_ind]))
    high_res_y = np.concatenate((high_res_y[bottom_ind:],high_res_y[:bottom_ind]))

    # recalculate indices with new indexing
    top_ind = top_ind-bottom_ind # note: negative index if top_ind<bottom_ind
    bottom_ind = bottom_ind-bottom_ind

    # create single array for apple coordinates
    lf_contour = np.column_stack((high_res_x, high_res_y))

    ##############################################################
    ### INTERPOLATE EACH SIDE WITH DESIRED NUMBER OF LANDMARKS ###
    ##############################################################

    # interpolate at desired resolution the left and right sides of the apple
    left_inter_x, left_inter_y = interpolation(lf_contour[bottom_ind:top_ind+1,0],lf_contour[bottom_ind:top_ind+1,1],res)
    right_inter_x, right_inter_y = interpolation(lf_contour[top_ind:,0],lf_contour[top_ind:,1],res)

    # the start of the right side and end of the left side
    # both contain the top landmark
    # delete the last point on the left side
    left_inter_x = np.delete(left_inter_x, -1)
    left_inter_y = np.delete(left_inter_y, -1)

    # BOTTOM OF APPLE IS INDEX 0
    # TOP INDEX IS RES-1 IF BOTH LEFT & RIGHT POINTS
    # TOTAL PSEUDOLANDMARKS IS 2*RES-1
    lf_pts_left = np.column_stack((left_inter_x, left_inter_y))
    lf_pts_right = np.column_stack((right_inter_x, right_inter_y))
    lf_pts = np.row_stack((lf_pts_left, lf_pts_right))

    ##########################################################
    ### ROTATE APPLES UPWARD AND SCALE SIZE TO CENTIMETERS ###
    ##########################################################

    top_point = lf_pts[res-1,:] # get top point
    bottom_point = lf_pts[0,:] # get bottom point

    # calculate angle between top. bottom, and an arbitrary reference
    ang = angle_between(top_point, bottom_point, (bottom_point[0]+1,bottom_point[1]) )

    # rotate points upwards
    rot_x, rot_y = rotate_points(lf_pts[:,0], lf_pts[:,1], ang) 
    rot_pts = np.column_stack((rot_x, rot_y))
    
    # calculate apple area in pixels^2
    lf_area_px2 = poly_area(rot_pts[:,0], rot_pts[:,1])
    
    # get px_cm
    #px_cm = np.sqrt(lf_area_px2/mdata["area"][lf])

    # scale apple into cm
    cm_lf = rot_pts/(81.93)
    
    # store the apple scaled into cm into the apple_cm_arr
    apple_cm_arr[lf,:,:] = cm_lf
```

<!-- Plot to be taken out for final analysis -->

### Only used for testing

```{python}
#| label: Pseudo-landmarks
# Plot each apple and check that it is working
# (remove this when working with a large number of leaves outside of this example)

plt.figure(figsize=(15,30))


for i in range(25):
    
    plt.subplot(5,5,i+1)
    plt.plot(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", lw=1)
    plt.plot([min(apple_cm_arr[i,:,0])-0.1,min(apple_cm_arr[i,:,0])-0.1],
            [apple_cm_arr[i,0,1], apple_cm_arr[i,0,1]+1], c="k", lw=0.5) # cm scale
    plt.scatter(apple_cm_arr[i,:,0], apple_cm_arr[i,:,1], c="k", s=4)
    plt.scatter(apple_cm_arr[i,0,0], apple_cm_arr[i,0,1])
    plt.scatter(apple_cm_arr[i,res-1,0], apple_cm_arr[i,res-1,1])
    
    plt.title(mdata["apple_id"][i], fontsize=8)
    
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
plt.suptitle(str(res*2-1) + " pseudo-landmarks")
plt.tight_layout()
plt.show()
```

<!-- The following sections of code are from the analysis -->

### Analyzing Apple Dimensions

Using placed pseudo-landmarks representing leaves that are rotated upwards and scaled in centimeters from `apple_cm_arr`, calculate the following:

-   `width`: difference in centimeters between minimum and maximum x values in an oriented apple
-   `length`: difference in centimeters between minimum and maximum y values in an oriented apple
-   `area`: area of the apple in centimeters squared
-   `solidity`: the ratio of area to convex hull area
-   `asymmetry`: the Procrustes distance between the superimposed left and right sides of a apple outline. Lower values are more symmetric. Higher values are more asymmetric.

Data is stored in the `mdata` dataframe.

```{python}
#| label: Analyzing Apple Dimensions
# lists to store variables
width_list = []
length_list = []
area_list = []
solidity_list = []
asymmetry_list = []

# for each apple . . .
for lf in range(len(apple_cm_arr)):
    
    # for calculating dimensions, we need non-scaled apple in centimeters
    curr_lf = apple_cm_arr[lf,:,:] # select current apple
    
    ############################
    ### CALCULATE DIMENSIONS ###
    ############################
    
    width = np.max(curr_lf[:,0])-np.min(curr_lf[:,0]) # calculate width
    length = np.max(curr_lf[:,1])-np.min(curr_lf[:,1]) # calculate length
    area = poly_area(curr_lf[:,0],curr_lf[:,1]) # calcualte area
    
    ##########################
    ### CALCULATE SOLIDITY ###
    ##########################
    
    hull = ConvexHull(curr_lf) # calculate convex hull of current apple
    vertices = hull.vertices # isolate vertex indices of convex hull
    convex_area = poly_area(curr_lf[vertices,0], curr_lf[vertices,1]) # calculate convex area
    solidity = area / convex_area # calculate solidity
    
    ##########################
    ### CALCULATE SYMMETRY ###
    ##########################
    
    left_side = curr_lf[:(res-1)+1,] # isolate left side of apple
    right_side = curr_lf[(res-1):,] # isolate right side of apple
    right_side = right_side[::-1] # reverse the right side to align indices with left

    # calculate procrustes distance between left and right side of apple
    s1, s2, distance = procrustes(left_side, right_side) 
    
    # store data in lists
    width_list.append(width)
    length_list.append(length)
    area_list.append(area)
    solidity_list.append(solidity)
    asymmetry_list.append(distance)
    
# add data to the mdata dataframe
mdata["width"] = width_list
mdata["length"] = length_list
mdata["area"] = area_list
mdata["solidity"] = solidity_list
mdata["asymmetry"] = asymmetry_list

# save output so you don't have to run the code all the time
mdata.to_csv("apple_metadata_0823.csv", index = False)
```

```{r}
setwd("C:/Users/User/Desktop/COOP02/code/apple_shape")
apple_traits <- read.csv("apple_metadata_0823.csv")
apple_traits <- apple_traits %>% 
  separate(file, into = c("BW", "nursery_id", "ending"), sep = "_", remove = FALSE) %>% 
  select(2, 4, 6:18)

apple_traits <- apple_traits %>% 
  group_by(nursery_id) %>% 
  mutate(across(all_of(c(10:14)), ~mean(.x, na.rm = TRUE), .names = "{col}"))

write.csv(apple_traits, file = "apple_metadata_avg_0903.csv")
```


```{r}
library(lme4)
library(lsmeans)
library(lattice)
library(pbkrtest)
library(tidyverse)

reml_design <- read.table("C:/Users/User/Desktop/COOP02/code/apple_shape/reml/20161024_ABC_design_grid_zm.txt", header = T)

apple_traits <- read.csv("apple_metadata_avg_0903.csv") %>% 
  select(2:16)

# Loading the metadata to match the design file
ph = apple_traits[match(reml_design[,"nursery_id"], apple_traits[,"nursery_id"]),] # Grab phenotype data in correct order
ph = cbind(reml_design, ph) #append phenotype data to reml tables
ph = ph[!is.na(ph[,"width"]),] # get rid of NAs
ph$apple_id = as.factor(as.character(ph$apple_id)) # make apple_id a factor

## check for each (will be slightly different)
length(unique(ph$nursery_id))
#743 nursery ids 
length(unique(ph$apple_id))
#534 unique cultivars (apple ids)


### WIDTH ---------
#Run the model
width_reml <- lmer(formula = width ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
width_reml_lsmeans <- lsmeans(width_reml , "apple_id")
width_reml_lsmeans_predict <- predict(width_reml_lsmeans)
names(width_reml_lsmeans_predict) = levels(ph$apple_id)

# name before saving them (apple_id and trait), once at the end with them all appended
write.table(width_reml_lsmeans_predict, "all_reml_lsmeans_predict.txt", sep="\t", row.names=T, quote=F, col.names=T) 

predict <- read.delim("all_reml_lsmeans_predict.txt", col.names = "width_reml_lsmeans_predict")

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(width ~ apple_id, data = ph, mean)
plot(mean_vals$width, width_reml_lsmeans_predict)
cor.test(mean_vals$width, width_reml_lsmeans_predict)
#cor width: 0.9958423 


### LENGTH ---------
#Run the model
length_reml <- lmer(formula = length ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
length_reml_lsmeans <- lsmeans(length_reml , "apple_id")
length_reml_lsmeans_predict <- predict(length_reml_lsmeans)
names(length_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, length_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(length ~ apple_id, data = ph, mean)
plot(mean_vals$length, length_reml_lsmeans_predict)
cor.test(mean_vals$length, length_reml_lsmeans_predict)
#cor length: 0.998904

### AREA ---------
#Run the model
area_reml <- lmer(formula = area ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
area_reml_lsmeans <- lsmeans(area_reml , "apple_id")
area_reml_lsmeans_predict <- predict(area_reml_lsmeans)
names(area_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, area_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(area ~ apple_id, data = ph, mean)
plot(mean_vals$area, area_reml_lsmeans_predict)
cor.test(mean_vals$area, area_reml_lsmeans_predict)
#cor area: 0.998154


### SOLIDITY ---------
#Run the model
solidity_reml <- lmer(formula = solidity ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
solidity_reml_lsmeans <- lsmeans(solidity_reml , "apple_id")
solidity_reml_lsmeans_predict <- predict(solidity_reml_lsmeans)
names(solidity_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, solidity_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(solidity ~ apple_id, data = ph, mean)
plot(mean_vals$solidity, solidity_reml_lsmeans_predict)
cor.test(mean_vals$solidity, solidity_reml_lsmeans_predict)
#cor solidity: 0.9989986


### ASYMMETRY ---------
#Run the model
asymmetry_reml <- lmer(formula = asymmetry ~ apple_id + ( 1 | Block) + ( 1 | Block:rGrid) + ( 1 | Block:cGrid) + ( 1 | Block:cGrid:rGrid), data= ph,  REML = TRUE) #apple_id is fixed effect

#Get the least-square means
asymmetry_reml_lsmeans <- lsmeans(asymmetry_reml , "apple_id")
asymmetry_reml_lsmeans_predict <- predict(asymmetry_reml_lsmeans)
names(asymmetry_reml_lsmeans_predict) = levels(ph$apple_id)

# Adding to the data
predict <- cbind(predict, asymmetry_reml_lsmeans_predict)

#Compare the predicted ls_means by cultivar to a simple arithmetic mean by cultivar
mean_vals = aggregate(asymmetry ~ apple_id, data = ph, mean)
plot(mean_vals$asymmetry, asymmetry_reml_lsmeans_predict)
cor.test(mean_vals$asymmetry, asymmetry_reml_lsmeans_predict)
#cor asymmetry: 1

predict <- tibble::rownames_to_column(predict, "apple_id")
write_delim(predict, file = "prediction_values.txt")
```



<!-- NOTE: will need to convert datasets that will be used from python to r. This can be done by using the py_to_r function from the reticulate library or import a file directly into R -->
```{r}
#| label: Pair Plot (by apple_id)
# insert pair plot here to visualize the differences in width, length, area, solidity, asymmetry

# load libraries
library(tidyverse)
library(GGally)

#setwd("C:/Users/User/Desktop/COOP02/code/apple_shape")
apple_data <- read.csv("apple_metadata_0821.csv")
apple_data$apple_id <- factor(apple_data$apple_id)

######## REML adjust
# but do we want to adjust all the data, or like just the averages 

# what if i average it first and then read it in?




apple_test <- apple_data %>% 
  group_by(apple_id) %>% 
  mutate(across(all_of(c(10:14)), ~mean(.x, na.rm = TRUE), .names = "aveg_{col}")) %>% 
  distinct(apple_id, .keep_all = TRUE)

# plotting for 549 different apple_id
plot <- ggpairs(apple_test, columns = 16:20, columnLabels = c("width", "length", "area", "solidity", "asymmetry"), aes(colour = apple_id, alpha = 0.5), upper = list(continuous = "points"))

#using all the data
for(i in 1:plot$nrow) {
  for(j in 1:plot$ncol){
    if(i == j) {  # Only modify the diagonal plots (density plots)
      p <- ggplot(apple_data, aes_string(x = names(apple_data)[j + 10])) + 
        geom_density(alpha = 0.5, fill = 'darkslategray1') + 
        theme(legend.position = "none")
      plot[i,j] <- p
    }
  }
}

print(plot)
```

<!-- The following sections are the Procrustes Analysis -->

### Procrustes Analysis

Perform a Procrustes analysis to translate, scale, and rotate apple shapes

-   Select number of pseudo-landmarks and dimensions
-   Calculate the GPA mean apple shape using the `gpa_mean` function
-   Align all apples to the GPA mean
-   Store Procrustes super-imposed apples in an array, `proc_arr`
-   Calculate a PCA for all possible axes and their variance (the number of apples)
-   Calculate a PCA for just the axes needed for reconstruction of eigenleaves for morphospace (probably 2)


<!-- this is where i need to load in the data with only the domestic apples -->

#### Calculate GPA Mean

```{python}
#| label: GPA Mean
landmark_num = (res*2)-1 # select number of landmarks
dim_num = 2 # select number of coordinate value dimensions

##########################
### CALCULATE GPA MEAN ###
##########################

mean_shape = gpa_mean(apple_cm_arr, landmark_num, dim_num)

################################
### ALIGN LEAVES TO GPA MEAN ###
################################

# array to store Procrustes aligned shapes
proc_arr = np.zeros(np.shape(apple_cm_arr)) 

for i in range(len(apple_cm_arr)):
  s1, s2, distance = procrustes(mean_shape, apple_cm_arr[i,:,:]) # calculate procrustes adjusted shape to ref for current apple
  proc_arr[i] = s2 # store procrustes adjusted shape to array
```

#### Calculate PC

```{python}
#| label: Calculate PC
#################################################
### FIRST, CALCULATE PERCENT VARIANCE ALL PCs ###
#################################################

######
PC_NUMBER = 78 # PC number = not number of apples, but features
#######

# use the reshape function to flatten to 2D
flat_arr = proc_arr.reshape(np.shape(proc_arr)[0], 
                                 np.shape(proc_arr)[1]*np.shape(proc_arr)[2]) 

pca_all = PCA(n_components=PC_NUMBER) 
PCs_all = pca_all.fit_transform(flat_arr) # fit a PCA for all data

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca_all.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca_all.explained_variance_ratio_[i]*100,1)) + 
          "%, " + str(round(pca_all.explained_variance_ratio_.cumsum()[i]*100,1)) + "%"  )
```

```{python}
#| label: Calculate Number of PCs
#################################################
### NEXT, CALCULATE THE DESIRED NUMBER OF PCs ###
#################################################

######
PC_NUMBER = 6 # PC number = 3, because PC 2 is asymmetry and we want to create morphospace for PC = 1 and 3
#######

pca = PCA(n_components=PC_NUMBER) 
PCs = pca.fit_transform(flat_arr) # fit a PCA for only desired PCs

# print out explained variance for each PC
print("PC: " + "var, " + "overall ") 
for i in range(len(pca.explained_variance_ratio_)):
    print("PC" + str(i+1) + ": " + str(round(pca.explained_variance_ratio_[i]*100,1)) + 
          "%, " + str(round(pca.explained_variance_ratio_.cumsum()[i]*100,1)) + "%"  )
    
# add PCs to dataframe for plotting
## MAY WANT TO USE MORE THAN 3 PCS
mdata["PC1"] = PCs[:,0]
mdata["PC2"] = PCs[:,1]
mdata["PC3"] = PCs[:,2]
mdata["PC4"] = PCs[:,3]
mdata["PC5"] = PCs[:,4]
mdata["PC6"] = PCs[:,5]

# save output with PC, could over write the other one if desired
mdata.to_csv("apple_metadata_0821.csv", index = False)
```
```{r}
######## adjust
# do we adjust the whole thing or do we adjust averages

```


#### Eigen Representation of PC

```{python}
#| label: Eigen Representation of PC
###################################################
### CREATE EIGEN REPRESENTATIONS OF FIRST 3 PCs ###
###################################################


# calculate standard deviations for each PC
PC1_std = mdata["PC1"].std()
PC2_std = mdata["PC2"].std()
PC3_std = mdata["PC3"].std()
PC4_std = mdata["PC4"].std()
PC5_std = mdata["PC5"].std()
PC6_std = mdata["PC6"].std()


# create list of lists of PC values to reconstruct

# std -3 -1.5 0 1.5 3
PC_vals = [[-3*PC1_std,0,0,0,0,0],
           [-1.5*PC1_std,0,0,0,0,0],
           [0*PC1_std,0,0,0,0,0],
           [1.5*PC1_std,0,0,0,0,0],
           [3*PC1_std,0,0,0,0,0],
           [0,-3*PC2_std,0,0,0,0],
           [0,-1.5*PC2_std,0,0,0,0],
           [0,0*PC2_std,0,0,0,0],
           [0,1.5*PC2_std,0,0,0,0],
           [0,3*PC2_std,0,0,0,0],
           [0,0,-3*PC3_std,0,0,0],
           [0,0,-1.5*PC3_std,0,0,0],
           [0,0,0*PC3_std,0,0,0],
           [0,0,1.5*PC3_std,0,0,0],
           [0,0,3*PC3_std,0,0,0],
           [0,0,0,-3*PC4_std,0,0],
           [0,0,0,-1.5*PC4_std,0,0],
           [0,0,0,0*PC4_std,0,0],
           [0,0,0,1.5*PC4_std,0,0],
           [0,0,0,3*PC4_std,0,0],
           [0,0,0,0,-3*PC5_std,0],
           [0,0,0,0,-1.5*PC5_std,0],
           [0,0,0,0,0*PC5_std,0],
           [0,0,0,0,1.5*PC5_std,0],
           [0,0,0,0,3*PC5_std,0],
           [0,0,0,0,0,-3*PC6_std],
           [0,0,0,0,0,-1.5*PC6_std],
           [0,0,0,0,0,0*PC6_std],
           [0,0,0,0,0,1.5*PC6_std],
           [0,0,0,0,0,3*PC6_std]]


plt.figure(figsize=(10,10))

counter = 1

for i in range(len(PC_vals)):
    
    # create inverse apple
    inv_leaf = pca.inverse_transform(np.array(PC_vals[i]))
    inv_x = inv_leaf[0::2] # select just inverse x vals
    inv_y = inv_leaf[1::2] # select just inverse y vals
    
    # plot inverse apple
    plt.subplot(6,5,counter)
    plt.fill(inv_x, inv_y, c="lightgray")
    plt.plot(inv_x, inv_y, c="gray")
    plt.gca().set_aspect("equal")
    plt.axis("off")
    
    
    counter += 1

plt.tight_layout()
plt.show()
#plt.savefig("eigen_apple_3&1_5.png")
```

### Morphospace and Linear Discriminant Analysis (By Genotype)

Visualize a morphospace and classify apples by the factor of genotype

-   Plot a morphospace using the inverse transform of the PCA
-   Perform a Linear Discriminant Analysis
-   Visualize LDA results as a confusion matrix
-   Plot out LDA scores
-   Note: the number of LDs is 1 minus the number of factor levels. In this example there are two genotypes, so there is only 1 LD
-   Note: the column names to plot the Linear Discriminants are manually renamed

#### Creating Morphospace

```{python}
## averaging the data
averaged = pd.read_csv("apple_metadata_0821.csv")
averaged = averaged.drop(['Unnamed: 0', 'dataset', 'file', 'px.cm', 'node'], axis = 1)

aveg_subset = averaged.groupby('apple_id').mean()
aveg_subset2 = aveg_subset.rename_axis('apple_id').reset_index()

aveg_subset2.to_csv("apple_metadata_0821_averages.csv", index = False)
```

```{r}
### combining averaged data
library(tidyverse)
library(readxl)
avgdata <- read.csv("apple_metadata_0821_averages.csv")
fulldata <- read.csv("apple_metadata_0821.csv")

dataset <- read_excel("Dataset_S1.xlsx", sheet = "phenotype_data")

combined <- avgdata %>% left_join(dataset, by = 'apple_id')
full <- fulldata %>% left_join(dataset, by = 'apple_id')

write.csv(combined, file = "apple_metadata_0821_averages.csv", row.names = FALSE)
write.csv(full, file = "apple_metadata_0821.csv", row.names = FALSE)
```

```{python}
#| label: Morphospace (PC1 and PC3)
##########################
### CREATE MORPHOSPACE ###
##########################

averaged = pd.read_csv("apple_metadata_0821_averages.csv")

# set plot parameters
plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 8 # set number of PC1 intervals
numPC3 = 5 # set number of PC3 intervals
hue = "species" # select the factor to color by
s = 0.06 # set the scale of the eigen apples
lf_col = "lightgray" # color of inverse eigen apples
lf_alpha = 0.5 # alpha of inverse eigen apple
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC3_vals = np.linspace( np.min(PCs[:,2]), np.max(PCs[:,2]), numPC3 )

for i in PC1_vals: # for each PC1 interval
    for j in PC3_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc3_val = j # select the current PC3 val

        # calculate the inverse eigenleaf
        inv_apple = pca.inverse_transform(np.array([pc1_val,0,pc3_val,0,0,0]))
        inv_x = inv_apple[0::2] # select just inverse x vals
        inv_y = inv_apple[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc3_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=averaged, x="PC1", y="PC3", hue=hue, s=pt_size, linewidth=pt_linewidth, alpha=pt_alpha)

# legend not helpful in this case
#plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9}, ncol=2)
plt.gca().get_legend().remove()
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC3, " + str(round(pca.explained_variance_ratio_[2]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_xlim([-0.125,0.09])
plt.gca().set_ylim([-0.06,0.07])
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
plt.show()

#plt.savefig("morphospace(1_3).png")
```

```{python}
#| label: Morphospace (PC1 and PC2)
##########################
### CREATE MORPHOSPACE ###
##########################

# set plot parameters

plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 8 # set number of PC1 intervals
numPC2 = 6 # set number of PC2 intervals
hue = "use" # select the factor to color by
s = 0.075 # set the scale of the eigen apples
lf_col = "lightgray" # color of inverse eigen apple
lf_alpha = 0.5 # alpha of inverse eigen apple
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC2_vals = np.linspace( np.min(PCs[:,1]), np.max(PCs[:,1]), numPC2 )

for i in PC1_vals: # for each PC1 interval
    for j in PC2_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc2_val = j # select the current PC2 val

        # calculate the inverse eigenleaf
        inv_appple = pca.inverse_transform(np.array([pc1_val,pc2_val,0,0,0,0]))
        inv_x = inv_apple[0::2] # select just inverse x vals
        inv_y = inv_apple[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc2_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=averaged, x="PC1", y="PC2", hue=hue, s=pt_size, linewidth=pt_linewidth, alpha=pt_alpha)

plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9}, ncol=1)
#plt.gca().get_legend().remove()
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC2, " + str(round(pca.explained_variance_ratio_[1]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_xlim([-0.12,0.09])
plt.gca().set_ylim([-0.05,0.05])
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
plt.show()

#plt.savefig("morphospace_1-2_area.png")
```

```{python}
#| label: Allometry Analysis
# To analyze apple size/allometry
# calculate apple area of each leaf in cm
# and replace 'area' column values in mdata

apple_areas_cm = []

for lf in range(len(apple_cm_arr)):
    
    apple_areas_cm.append(poly_area(apple_cm_arr[lf,:,0], apple_cm_arr[lf,:,1]))
    
mdata["area_cm"] = apple_areas_cm

# export this one with the updated area
mdata.to_csv("apple_metadata_0821.csv", index = False)
```

```{python}
## averaging the data
averaged = pd.read_csv("apple_metadata_0821.csv")
averaged = averaged.drop(['Unnamed: 0', 'file', 'dataset', 'px.cm', 'node'], axis = 1)

aveg_subset = averaged.groupby('apple_id').mean()
aveg_subset2 = aveg_subset.rename_axis('apple_id').reset_index()

aveg_subset2.to_csv("apple_metadata_0821_averages.csv", index = False)
```

```{r}
### combining averaged data
library(tidyverse)
library(readxl)
avgdata <- read.csv("apple_metadata_0821_averages.csv")
fulldata <- read.csv("apple_metadata_0821.csv")

dataset <- read_excel("Dataset_S1.xlsx", sheet = "phenotype_data")

combined <- avgdata %>% left_join(dataset, by = 'apple_id')
full <- fulldata %>% left_join(dataset, by = 'apple_id')

write.csv(combined, file = "apple_metadata_0821_averages.csv", row.names = FALSE)
write.csv(full, file = "apple_metadata_0821.csv", row.names = FALSE)
```

```{python}
#| label: Morphospace 2
##########################
### CREATE MORPHOSPACE ###
##########################

# set plot parameters
averaged = pd.read_csv("apple_metadata_0821_averages.csv")

plot_length= 20 # plot length in inches
plot_width= 20 # plot length in inches
numPC1 = 10 # set number of PC1 intervals
numPC3 = 5 # set number of PC3 intervals
hue = "area_cm" # select the factor to color by
s = 0.075 # set the scale of the eigen apples
lf_col = "lightgray" # color of inverse eigen apple
lf_alpha = 0.5 # alpha of inverse eigen apple
pt_size = 20 # size of data points
pt_linewidth = 0 # lw of data points, set to 0 for no edges
pt_alpha = 0.6 # alpha of the data points
ax_label_fs = 12 # font size of the x and y axis titles
ax_tick_fs = 8 # font size of the axis ticks
face_col = "white" # color of the plot background
grid_alpha = 0.5 # set the alpha of the grid
title = "Procrustean morphospace" # set title

plt.figure(figsize=(plot_length, plot_width))

# note that PC2 is asymmetry and always = 0
PC1_vals = np.linspace( np.min(PCs[:,0]), np.max(PCs[:,0]), numPC1 ) # create PC intervals
PC3_vals = np.linspace( np.min(PCs[:,2]), np.max(PCs[:,2]), numPC3 )

for i in PC1_vals: # for each PC1 interval
    for j in PC3_vals: # for each PC2 interval
        
        pc1_val = i # select the current PC1 val
        pc3_val = j # select the current PC3 val

        # calculate the inverse eigenleaf
        inv_apple = pca.inverse_transform(np.array([pc1_val,0,pc3_val,0,0,0]))
        inv_x = inv_apple[0::2] # select just inverse x vals
        inv_y = inv_apple[1::2] # select just inverse y vals
        
        # plot the inverse eigenleaf
        plt.fill(inv_x*s+pc1_val, inv_y*s+pc3_val, c=lf_col, alpha=lf_alpha)
   
# plot the data on top of the morphospace
sns.scatterplot(data=averaged, 
                x="PC1", 
                y="PC3", 
                hue=hue, 
                s=pt_size, 
                linewidth=pt_linewidth, 
                alpha=pt_alpha, 
                palette = "inferno_r")

#plt.legend(bbox_to_anchor=(1.00, 1.02), prop={'size': 8.9})
xlab = "PC1, " + str(round(pca.explained_variance_ratio_[0]*100,1)) + "%"
ylab = "PC3, " + str(round(pca.explained_variance_ratio_[2]*100,1)) + "%"
plt.xlabel(xlab, fontsize=ax_label_fs)
plt.ylabel(ylab, fontsize=ax_label_fs)
plt.xticks(fontsize=ax_tick_fs)
plt.yticks(fontsize=ax_tick_fs)
plt.gca().set_aspect("equal")
plt.gca().set_facecolor(face_col)
plt.grid(alpha=grid_alpha)
plt.gca().set_axisbelow(True)
plt.title(title)
plt.show()
```

<!-- Linear Discriminant Analysis to be completed here (can be done in R): -->

<!-- Plots that may want to be analyzed: score by genotype, score by size, etc. -->

```{r}
#| label: Pair Plot (by use)
# load libraries
library(tidyverse)
library(GGally)

apple_data <- read.csv("apple_metadata_0821.csv")
apple_data$apple_id <- factor(apple_data$apple_id)

apple_test <- apple_data %>% 
  filter(use == c("dessert", "cider"))

# plotting for 249 cider apples and 2303 dessert apples
ggpairs(apple_test, columns = 11:15, aes(colour = use, alpha = 0.35), upper = list(continuous = "points")) +
  scale_fill_manual(values = c('#2e6b52', '#d98fe3')) +
  scale_color_manual(values = c('#2e6b52', '#d98fe3'))
```


```{r}
library(GGally)
library(tidyverse)

apple_metadata <- read.csv("apple_metadata_0821_averages.csv")
apple_metadata$apple_id <- factor(apple_metadata$apple_id)

plot2 <- ggpairs(apple_metadata, columns = 6:16, aes(colour = apple_id, alpha = 0.35), upper = list(continuous = "points"))

ggsave("density_plot.jpg", plot = plot2, width = 15, height = 10, units = "in")
```
